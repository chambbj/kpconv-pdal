defaults:
  - hydra/launcher: joblib
hydra:
  run:
    dir: ./outputs
  sweep:
    subdir: ./${method}/${depth}/${multiplier}

###################
# KPConv parameters
###################

# Number of CPU threads for the input pipeline
input_threads: 10

# Radius of the input sphere
in_radius: 50.0

# Number of kernel points
num_kernel_points: 15

# Size of the first subsampling grid in meter
first_subsampling_dl: 2.0

# Radius of convolution in "number grid cell". (2.5 is the standard value)
conv_radius: 2.5

# Radius of deformable convolution in "number grid cell". Larger so that deformed kernel can spread out
deform_radius: 6.0

# Radius of the area of influence of each kernel point in "number grid cell". (1.0 is the standard value)
KP_extent: 1.2

# Behavior of convolutions in ('constant', 'linear', 'gaussian')
KP_influence: linear

# Aggregation function of KPConv in ('closest', 'sum')
aggregation_mode: sum

# Choice of input features
first_features_dim: 128
in_features_dim: 5

# Can the network learn modulations
modulated: False

# Batch normalization parameters
use_batch_norm: True
batch_norm_momentum: 0.02

# Deformable offset loss
# 'point2point' fitting geometry by penalizing distance from deform point to input points
# 'point2plane' fitting geometry by penalizing distance from deform point to input point triplet (not implemented)
deform_fitting_mode: point2point
deform_fitting_power: 1.0              # Multiplier for the fitting/repulsive loss
deform_lr_factor: 0.1                  # Multiplier for learning rate applied to the deformations
repulse_extent: 1.2                    # Distance of repulsion for deformed kernel points

#####################
# Training parameters
#####################

# Maximal number of epochs
max_epoch: 500

# Learning rate management
# learning_rate: 1e-2
momentum: 0.98
# lr_decays = {i: 0.1 ** (1 / 150) for i in range(1, max_epoch)}
grad_clip_norm: 100.0

# Number of batch
batch_num: 8

# Number of steps per epochs
epoch_steps: 500

# Number of validation examples per epoch
validation_size: 50

# Number of epoch between each checkpoint
checkpoint_gap: 50

# Augmentations
augment_scale_anisotropic: True
augment_symmetries: [True, False, False]
augment_rotation: vertical
augment_scale_min: 0.9
augment_scale_max: 1.1
augment_noise: 0.05
augment_color: 0.8

# The way we balance segmentation loss
#   > 'none': Each point in the whole batch has the same contribution.
#   > 'class': Each class has the same contribution (points are weighted according to class balance)
#   > 'batch': Each cloud in the batch has the same contribution (points are weighted according cloud sizes)
segloss_balance: none

# Do we nee to save convergence
saving: True
saving_path: None

# Dataset folder
path: /home/chambbj/data/ml-datasets/US3D/train-single-file-prototype-kpconv/
writer: /home/chambbj/data/tensorboard-runs/new/hydra

ignored_labels: [1,3,4,7,8,10,11,12,13,14,15,16,18]
test_path_suffix: test

# Prediction param
chosen_log: /home/chambbj/code/kpconv-pytorch-pdal/outputs/None
prediction_validation_size: 200
prediction_input_threads: 10